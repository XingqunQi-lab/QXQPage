
<!doctype html>
<html>

<head>
<title>Xingqun Qi</title>
<link rel="icon" type="image/webp" href="imgs/icon.webp">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Xingqun Qi, HKUST, The Hong Kong University of Science and Technology, AI, PhD, Á•ÅÊòüÁæ§ÔºåÈ¶ôÊ∏ØÁßëÊäÄÂ§ßÂ≠¶"> 
<meta name="description" content="Xingqun Qi's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-137722442-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        // var e = document.getElementById(id);
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }

    function toggle_research_vis(target, tabElement) {
        var is_current_summary = 0;

        var e = document.getElementsByClassName("research_summary");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                if (e[i].style.display == "inline") {
                    is_current_summary = 1;
                } else {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("progress_button");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target + "_button") {
                if (is_current_summary == 0) {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("goal_tabs");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id != target + "_goal_tabs") {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_research_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "research_tab";
        }
        tabElement.className = "highlight_research_tab"
    }

    function toggle_goal_vis(id, goal_tabs_id, goal) {
        var e = document.getElementById(id);
        e.style.display = "none";
        var goal_tabs = document.getElementById(goal_tabs_id);
        goal_tabs.style.display = "inline";

        var goal_tab = document.getElementById(goal + "_tab");
        toggle_goal_progress_vis(goal_tab);
        // add_goal_progress_div(goal);
    }

    function toggle_goal_progress_vis(tabElement) {
        var target = tabElement.id;
        target = target.substring(0, target.length - 4);

        var e = document.getElementsByClassName("goal_progress");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                e[i].style.display = "inline";
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_goal_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "goal_tab";
        }
        tabElement.className = "highlight_goal_tab"

        add_goal_progress_div(target);
    }

    function add_goal_progress_div(goal) {
        var e = document.getElementById(goal);
        if (e && e.children.length == 0) {
            var children = Array.from(document.getElementsByClassName(goal));
            for (var i = 0; i < children.length; i++) {
                var cloned_div = children[i].cloneNode(true);
                cloned_div.className = "publication_container";
                e.appendChild(cloned_div);
            }
        }
    }
</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Xingqun Qi (Á•ÅÊòüÁæ§)<h1>
				</div>

                <h3 class="title">PhD Candidate</h3> </h1>

				<p>
                    
                    Academy of Interdisciplinary Studies (AIS)</br>
                    The Hong Kong University of Science and Technology (HKUST)</br></br>
		    <b>Research Interests:</b>3D Human Motion Modeling, Egocentric View Analysis, Medical Image Analysis</br>
                    Email: <a href="mailto:xingqunqi@gmail.com">xingqunqi dot gmail dot com </a></br>
					</br>
                    [<a href="https://scholar.google.com.hk/citations?user=3tO41a8AAAAJ&hl=zh-CN">Google Scholar</a>] </br> 
				</p>

			</td>
			<td width="25%">
				<img src="photo/homepage.jpg" width="70%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>Short Bio</h2> 

<div style="display: flex; margin-bottom: -10px">
    <p>
	    I am a PhD candidate at The Hong Kong University of Science and Technology (HKUST), supervised by <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo">Prof. Yike Guo</a> and co-supervised by <a href="https://emia.hkust.edu.hk/people/detail/qifeng-liu-liuqifeng">Prof. Qifeng Liu</a>. </br>
	    I received my Master's degree from the Beijing University of Posts and Telecommunications in June of 2022, supervised by <a href="https://caifeng-shan.github.io/">Prof. Caifeng Shan</a> and <a href="https://scholar.google.com.hk/citations?user=Ti7NNqMAAAAJ&hl=zh-CN">Dr. Muyi Sun</a>. I received my Bachelor's degree from both the Beijing University of Posts and Telecommunications and the Queen Mary University of London.</br>
	    My research focuses on 3D Human Motion Modeling, Egocentric View Analysis, and Medical Image Analysis. 
        </br>
        I work closely with <a href="https://whluo.github.io/">Prof. Wenhan Luo</a>, <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Prof. Shanghang Zhang</a>, and <a href="https://scholar.google.com.hk/citations?user=Ti7NNqMAAAAJ&hl=zh-CN">Dr. Muyi Sun</a> </br>
		</br>
		<!--Moreover, I collaborate closely with <a href="https://scholar.google.com/citations?user=NYLsVscAAAAJ">Lincheng Li</a>, <a href="https://gogoduck912.github.io/">Peike Li</a>, <a href="https://people.csiro.au/w/d/dadong-wang.aspx">Dadong Wang</a>, and <a href="https://xingqunqi-lab.github.io/QXQPage/">Xingqun Qi</a>. Very fortunate to encounter these great collaborators!-->
		</br>
		<b>I'd like to connect with anyone who's passionate about research. Excited for some great intellectual discussions!</b>
    </p>
</div>

<h2>News</h2>

<ul>
	<li>
        <div class="marker">[2024-03] One paper accepted by IEEE JBHI.</div>
    </li>
    <li>
        <div class="marker">[2024-03] One paper accepted to CVPR 2024.</div>
    </li>
	<li>
        <div class="marker">[2023-12] One paper accepted to IEEE TNNLS.</div>
    </li>
	<li>
        <div class="marker">[2023-10] One paper accepted to IEEE TMM.</div>
    </li>
	<li>
        <div class="marker">[2023-05] One paper accepted to ACM MM 2023.</div>
    </li>
    <li>
        <div class="marker">[2023-03] One paper accepted to CVPR 2023.</div>
    </li>
    <li>
        <div class="marker">[2022-12] One paper accepted to IEEE TMM.</div>
    </li>
    <li>
        <div class="marker">[2022-03] One paper accepted to CVPR 2022.</div>
    </li>
    </div>
</ul>

<div class="show_button">
    <a href="javascript:toggle_vis('news')" id="showText">[Show more]</a>
</div>


<h2>Preprints
    <span style="font-size: 50%;">(* denotes equal contribution)</span>
</h2>

<!-- <div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_Img/TMM.jpg">
    </div>
    <div class="publication_title">
        BAVS: bootstrapping audio-visual segmentation by integrating foundation knowledge</br>
        <b>Chen Liu</b>, Peike Li, Hu Zhang, Lincheng Li, Zi Huang, Dadong Wang, Xin Yu</br>
        [<a href="https://arxiv.org/pdf/2308.10175.pdf">Paper</a>][<a href="https://yenanliu.github.io/AVSS.github.io/">Project Page</a>]
    </div>
</div> -->

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/NeurIPS_2023/EMOTION.png">
    </div>
    <div class="publication_title">
        Emotiongesture: Audio-driven diverse emotional co-speech 3d gesture generation</br>
        <b>Xingqun Qi*</b>, Chen Liu*, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu</br>
        [<a href="https://arxiv.org/pdf/2305.18891v2.pdf">Paper</a>][<a href="https://xingqunqi-lab.github.io/Emotion-Gesture-Web/">Project Page</a>]
    </div>
</div>


<h2>
    Publications
    <span style="font-size: 50%;">(* denotes equal contribution)</span>
</h2>

<div class="newline_bg">
    <h3>2024</h3>
</div>

<div class="Medical Image Analysis">
    <div class="publication_image">
        <img src="papers_teaser/TMI_2023/GKD.png">
    </div>
    <div class="publication_title">
        Exploring Generalizable Distillation for Efficient Medical Image Segmentation </br>
        <b>Xingqun Qi</b>, Zhuojie Wu, Wenxuan Zou, Min Ren, Yifan Gao, Muyi Sun, Shanghang Zhang, Caifeng Shan, Zhenan Sun
</br>
        IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) 2024.</br>
<!--         (üî•Comming Soon!) </br> -->
	[<a href="https://arxiv.org/pdf/2311.17532.pdf">Paper</a>][<a href="https://xingqunqi-lab.github.io/Emo-Transition-Gesture/">Project Page</a>]
    </div>
</div>
						
<div class="Co-speech Gesture">
    <div class="publication_image">
        <img src="papers_teaser/CVPR24/CVPR_24_teaser.png">
    </div>
    <div class="publication_title">
        Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation </br>
        <b>Xingqun Qi</b>, Jiahan Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</br>
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2024.</br>
<!--         (üî•Comming Soon!) </br> -->
	[<a href="https://arxiv.org/pdf/2311.17532.pdf">Paper</a>][<a href="https://xingqunqi-lab.github.io/Emo-Transition-Gesture/">Project Page</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2023</h3>
</div>
						
<div class="TNNLS_2023">
    <div class="publication_image">
        <img src="papers_teaser/TNNLS_2023/GRAPH.png">
    </div>
    <div class="publication_title">
        Biphasic face photo-sketch synthesis via semantic-driven generative adversarial network with graph representation learning/br>
        <b>Xingqun Qi*</b>, Muyi Sun*, Zijian Wang, Jiaming Liu, Qi Li, Fang Zhao, Shanghang Zhang, Caifeng Shan</br>
        IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS) 2023.</br>
        [<a href="https://ieeexplore.ieee.org/document/10356848">Paper</a>]
    </div>
</div>

					
<div class="TMM_2023">
    <div class="publication_image">
        <img src="papers_teaser/TMM_2023/TMM_teaser.png">
    </div>
    <div class="publication_title">
        Calligraphy Font Generation via Explicitly Modeling Location-aware Glyph Component Deformations</br>
        Minda Zhao*, <b>Xingqun Qi*</b>, Zhiping Hu, Lincheng Li, Yongqiang Zhang, Zi Huang, Xin Yu</br>
        IEEE Transactions on Multimedia (IEEE TMM) 2023.</br>
        [<a href="https://ieeexplore.ieee.org/document/10356848">Paper</a>]
    </div>
</div>


<div class="publication_container AVS_MS">
    <div class="publication_image">
        <img src="papers_teaser/MM_teaser.png">
    </div>
    <div class="publication_title">
        Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics</br>
        Chen Liu, Peike Li, <b>Xingqun Qi</b>, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</br>
        ACM International Conference on Multimedia (ACM MM) 2023.</br>
        [<a href="https://arxiv.org/pdf/2307.16620.pdf">Paper</a>]
</div>
</div>

<div class="CVPR_2023">
    <div class="publication_image">
        <img src="papers_teaser/CVPR_2023/CVPR_2023_teaser3.jpg">
    </div>
    <div class="publication_title">
        Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</br>
        <b>Xingqun Qi</b>, Chen Liu, Muyi Sun, Lingchen Li, Changjie Fan, Xin Yu</br>
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2023.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Diverse_3D_Hand_Gesture_Prediction_From_Body_Dynamics_by_Bilateral_CVPR_2023_paper.pdf">Paper</a>]
    </div>
</div>


<div class="newline_bg">
    <h3>2022</h3>
</div>

<div class="TMM 2022">
    <div class="publication_image">
        <img src="papers_teaser/TMI_2022/graphflow.png">
    </div>
    <div class="publication_title">
        Graph flow: Cross-layer graph flow distillation for dual efficient medical image segmentation</br>
        Wenxuan Zou, <b>Xingqun Qi</b>, Wanting Zhou, Muyi Sun, Zhenan Sun, Caifeng Shan/br>
        IEEE Transactions on Medical Imaging (IEEE TMM) 2022.</br>
        [<a href="https://ieeexplore.ieee.org/abstract/document/9963578">Paper</a>]
	</div>
</div>

<div class="CVPR_2022">
    <div class="publication_image">
        <img src="papers_teaser/CVPR_2022/PIG.png">
    </div>
    <div class="publication_title">
        Self-supervised correlation mining network for person image generation</br>
        Zijian Wang, <b>Xingqun Qi</b>, Kun Yuan, Muyi Sun
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2022.</br>
        [<a href="https://www.mdpi.com/2077-0472/12/4/452">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2021</h3>
</div>

<div class="Medical Image Analysis JBHI">
    <div class="publication_image">
        <img src="papers_teaser/JBHI_2021/FANET.png">
    </div>
    <div class="publication_title">
        Accurate Retinal Vessel Segmentation in Color Fundus images via Fully Attention-based Networks </br>
        Kaiqi Li*, <b>Xingqun Qi*</b>, Yiwen Luo, Zeyi Yao, Xiaoguang Zhou, Muyi Sun
</br>
        IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) 2021.</br>
	[<a href="https://ieeexplore.ieee.org/abstract/document/9210783">Paper</a>]
    </div>
</div>

						
<h2>Competitions</h2>

<ul>                                                                   -->
    <li>                                                                        
        <div class="marker">Second Place of the ChaLearn Sign Spotting Challenge Challenge</div> <div>ECCV, 2022</div>
    </li>                                                                       
</ul>
 
<h2>Services</h2>

<ul>                                                                
    <li>
        <div class="marker">Conference Reviewer: CVPR, ACM MM, ACCV (outstanding reviewer in 2022)</div>
    </li>                                                                       
    <li>
        <div class="marker">Journal Reviewer: IJCV, TNNLS, JBHI, TCBB</div>
    </li>                                                                       
</ul>

</div>

</div>
</body>
</html>
