
<!doctype html>
<html>

<head>
<title>Xingqun Qi</title>
<link rel="icon" type="image/webp" href="imgs/icon.webp">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Xingqun Qi, HKUST, The Hong Kong University of Science and Technology, AI, PhD, 祁星群，香港科技大学"> 
<meta name="description" content="Xingqun Qi's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-137722442-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        // var e = document.getElementById(id);
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }

    function toggle_research_vis(target, tabElement) {
        var is_current_summary = 0;

        var e = document.getElementsByClassName("research_summary");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                if (e[i].style.display == "inline") {
                    is_current_summary = 1;
                } else {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("progress_button");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target + "_button") {
                if (is_current_summary == 0) {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("goal_tabs");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id != target + "_goal_tabs") {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_research_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "research_tab";
        }
        tabElement.className = "highlight_research_tab"
    }

    function toggle_goal_vis(id, goal_tabs_id, goal) {
        var e = document.getElementById(id);
        e.style.display = "none";
        var goal_tabs = document.getElementById(goal_tabs_id);
        goal_tabs.style.display = "inline";

        var goal_tab = document.getElementById(goal + "_tab");
        toggle_goal_progress_vis(goal_tab);
        // add_goal_progress_div(goal);
    }

    function toggle_goal_progress_vis(tabElement) {
        var target = tabElement.id;
        target = target.substring(0, target.length - 4);

        var e = document.getElementsByClassName("goal_progress");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                e[i].style.display = "inline";
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_goal_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "goal_tab";
        }
        tabElement.className = "highlight_goal_tab"

        add_goal_progress_div(target);
    }

    function add_goal_progress_div(goal) {
        var e = document.getElementById(goal);
        if (e && e.children.length == 0) {
            var children = Array.from(document.getElementsByClassName(goal));
            for (var i = 0; i < children.length; i++) {
                var cloned_div = children[i].cloneNode(true);
                cloned_div.className = "publication_container";
                e.appendChild(cloned_div);
            }
        }
    }
</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Xingqun Qi (祁星群)<h1>
				</div>

                <h3 class="title">PhD Candidate in Artificial Intelligence</h3> </h1>

				<p>
                    
                    Academy of Interdisciplinary Studies (AIS)</br>
                    The Hong Kong University of Science and Technology (HKUST)</br></br>
		    <b>Research Interests: </b> Human Motion Modeling, Medical Image Analysis, Biometrics</br>
                    Email: <a href="mailto:xingqunqi@gmail.com">xingqunqi dot gmail dot com </a></br>
					</br>
                    [<a href="https://scholar.google.com.hk/citations?user=3tO41a8AAAAJ&hl=zh-CN">Google Scholar</a>] </br> 
				</p>

			</td>
			<td width="25%">
				<img src="photo/homepage.jpg" width="70%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2><font color="#4682B4">Short Bio</font></h2> 

<div style="display: flex; margin-bottom: -10px">
    <p>
	    I am a Ph.D. candidate at The Hong Kong University of Science and Technology (HKUST), supervised by <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo">Prof. Yike Guo</a> and co-supervised by <a href="https://emia.hkust.edu.hk/people/detail/qifeng-liu-liuqifeng">Prof. Qifeng Liu</a>. </br>
	    I received my Master's degree from the Beijing University of Posts and Telecommunications in June of 2022, supervised by <a href="https://caifeng-shan.github.io/">Prof. Caifeng Shan</a> and <a href="https://scholar.google.com.hk/citations?user=Ti7NNqMAAAAJ&hl=zh-CN">Dr. Muyi Sun</a>. I received my Bachelor's degree from both the Beijing University of Posts and Telecommunications and the Queen Mary University of London.</br>
	    My research focuses on 3D Human Motion Modeling and Medical Image Analysis. 
        </br></br>
         Currently, I work closely with <a href="https://whluo.github.io/">Prof. Wenhan Luo</a>, <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Prof. Shanghang Zhang</a>, and <a href="https://scholar.google.com.hk/citations?user=Ti7NNqMAAAAJ&hl=zh-CN">Dr. Muyi Sun.</a> </br>
	 Moreover, I collaborate closely with <a href="https://sites.google.com/view/xinyus-homepage/Home">Prof. Xin Yu</a>, and <a href="https://yenanliu.github.io/PersonalPage/">Chen Liu</a> during my gap year from June 2022 to May 2023 (waiting for visa). Very fortunate to encounter these great collaborators.
		</br>
		<!--<b>I'd like to connect with anyone who's passionate about research. Excited for some great intellectual discussions!</b>-->
    </p>
</div>

<h2><font color="#4682B4">News</font></h2>

<ul>
	<li>
        <div class="marker">[2025-01] One paper accepted by ICLR 2025 (<font color="#FF0000">Spotlight</font>).</div>
    </li>
	<li>
        <div class="marker">[2024-05] One paper accepted by IEEE TMM.</div>
    </li>
	<li>
        <div class="marker">[2024-03] One paper accepted by IEEE JBHI.</div>
    </li>
    <li>
        <div class="marker">[2024-03] One paper accepted to CVPR 2024.</div>
    </li>
	<li>
        <div class="marker">[2023-12] One paper accepted to IEEE TNNLS.</div>
    </li>
	<li>
        <div class="marker">[2023-10] One paper accepted to IEEE TMM.</div>
    </li>
<!-- 	<li>
        <div class="marker">[2023-05] One paper accepted to ACM MM 2023.</div>
    </li> -->
    <li>
        <div class="marker">[2023-03] One paper accepted to CVPR 2023.</div>
    </li>
    <!--<li>
        <div class="marker">[2022-12] One paper accepted to IEEE TMI.</div>
    </li>
    <li>
        <div class="marker">[2022-03] One paper accepted to CVPR 2022.</div>
    </li>
    </div>-->
</ul>

<div class="show_button">
    <a href="javascript:toggle_vis('news')" id="showText">[Show more]</a>
</div>


<h2><font color="#4682B4">Preprints</font>
    <span style="font-size: 70%;">(* denotes equal contribution; # denotes corresponding author)</span>
</h2>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/AVU/teaser.png">
    </div>
    <div class="publication_title">
        Audio-Visual Understanding: Towards Fine-Grained Audio-Visual Learning with Region-Aware Sound Source Understanding</br>
        Muyi Sun, Hong Wang, Chen Su, Yixuan Wang, Song Wang, Man Zhang, <b>Xingqun Qi#</b>, Qi Li, Zhenan Sun</br>
<!-- 	    IEEE Transactions on Multimedia (IEEE TMM) 2023.</br> -->
<!--         [<a href="https://arxiv.org/pdf/2405.16874">Paper</a>][<a href="https://mattie-e.github.io/GES-X/">Project Page</a>] -->
    </div>
</div>


						
<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/NIPS_2024/teaser.png">
    </div>
    <div class="publication_title">
        CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild</br>
        <b>Xingqun Qi*</b>, Hengyuan Zhang*, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo</br>
<!-- 	    IEEE Transactions on Multimedia (IEEE TMM) 2023.</br> -->
        [<a href="https://arxiv.org/pdf/2405.16874">Paper</a>][<a href="https://mattie-e.github.io/GES-X/">Project Page</a>]
    </div>
</div>



<h2>
    <font color="#4682B4">Publications</font>
    <span style="font-size: 70%;">(* denotes equal contribution; # denotes corresponding author)</span>
</h2>

<div class="newline_bg">
    <h3>2025</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/Co3/teaser.png">
    </div>
    <div class="publication_title">
        Co3Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</br>
        <b>Xingqun Qi*</b>, Yatian Wang*, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo</br>
	    The International Conference on Learning Representations (ICLR <font color="#FF0000">Spotlight</font>) 2025.</br>
	    [<a href="https://ieeexplore.ieee.org/document/10543093">Paper</a>][<a href="https://xingqunqi-lab.github.io/Emotion-Gesture-Web/">Project Page</a>]
    </div>
</div> 
						
<div class="newline_bg">
    <h3>2024</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/NeurIPS_2023/EMOTION.png">
    </div>
    <div class="publication_title">
        Emotiongesture: Audio-driven diverse emotional co-speech 3d gesture generation</br>
        <b>Xingqun Qi*</b>, Chen Liu*, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu</br>
	    IEEE Transactions on Multimedia (IEEE TMM) 2024.</br>
        [<a href="https://openreview.net/pdf?id=VaowElpVzd">Paper</a>][<a href="https://xingqunqi-lab.github.io/QXQPage/">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/TMI_2023/GKD.png">
    </div>    <div class="publication_title">
        Exploring Generalizable Distillation for Efficient Medical Image Segmentation</br>
        <b>Xingqun Qi</b>, Zhuojie Wu, Wenxuan Zou, Min Ren, Yifan Gao, Muyi Sun, Shanghang Zhang, Caifeng Shan, Zhenan Sun </br>
        IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) 2024.</br>
	[<a href="https://ieeexplore.ieee.org/document/10491241">Paper</a>][<a href="https://github.com/XingqunQi-lab/GKD-Framework">Project Page</a>]
    </div>
</div>
						
<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/CVPR24/CVPR_24_teaser.png">
    </div>
    <div class="publication_title">
        Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation </br>
        <b>Xingqun Qi</b>, Jiahan Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo
</br>
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2024.</br>
	[<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_Weakly-Supervised_Emotion_Transition_Learning_for_Diverse_3D_Co-speech_Gesture_Generation_CVPR_2024_paper.html">Paper</a>][<a href="https://xingqunqi-lab.github.io/Emo-Transition-Gesture/">Project Page</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2023</h3>
</div>
						
<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/TNNLS_2023/GRAPH.png">
    </div>
    <div class="publication_title">
        Biphasic face photo-sketch synthesis via semantic-driven generative adversarial network with graph representation learning</br>
        <b>Xingqun Qi*</b>, Muyi Sun*, Zijian Wang, Jiaming Liu, Qi Li, Fang Zhao, Shanghang Zhang, Caifeng Shan</br>
        IEEE Transactions on Neural Networks and Learning Systems (IEEE TNNLS) 2023.</br>
        [<a href="https://ieeexplore.ieee.org/document/10365567">Paper</a>]
    </div>
</div>

					
<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/TMM_2023/TMM_teaser.png">
    </div>
    <div class="publication_title">
        Calligraphy Font Generation via Explicitly Modeling Location-aware Glyph Component Deformations</br>
        Minda Zhao*, <b>Xingqun Qi*</b>, Zhiping Hu, Lincheng Li, Yongqiang Zhang, Zi Huang, Xin Yu</br>
        IEEE Transactions on Multimedia (IEEE TMM) 2023.</br>
        [<a href="https://ieeexplore.ieee.org/document/10356848">Paper</a>]
    </div>
</div>


<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/MM_teaser.png">
    </div>
    <div class="publication_title">
        Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics</br>
        Chen Liu, Peike Li, <b>Xingqun Qi</b>, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</br>
        ACM International Conference on Multimedia (ACM MM) 2023.</br>
        [<a href="https://arxiv.org/pdf/2307.16620.pdf">Paper</a>]
</div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/CVPR_2023/CVPR_2023_teaser3.jpg">
    </div>
    <div class="publication_title">
        Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</br>
        <b>Xingqun Qi</b>, Chen Liu, Muyi Sun, Lingchen Li, Changjie Fan, Xin Yu</br>
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2023.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Diverse_3D_Hand_Gesture_Prediction_From_Body_Dynamics_by_Bilateral_CVPR_2023_paper.pdf">Paper</a>]
    </div>
</div>


<div class="newline_bg">
    <h3>2022</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/TMI_2022/graphflow.png">
    </div>
    <div class="publication_title">
        Graph flow: Cross-layer graph flow distillation for dual efficient medical image segmentation</br>
        Wenxuan Zou, <b>Xingqun Qi</b>, Wanting Zhou, Muyi Sun, Zhenan Sun, Caifeng Shan</br>
        IEEE Transactions on Medical Imaging (IEEE TMI) 2022.</br>
        [<a href="https://ieeexplore.ieee.org/abstract/document/9963578">Paper</a>]
	</div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/CVPR_2022/PIG.png">
    </div>
    <div class="publication_title">
        Self-supervised correlation mining network for person image generation</br>
        Zijian Wang, <b>Xingqun Qi</b>, Kun Yuan, Muyi Sun </br>
        IEEE/CVF Computer Vision and Pattern Recognition (IEEE/CVF CVPR) 2022.</br>
        [<a href="https://www.mdpi.com/2077-0472/12/4/452">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2021</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="papers_teaser/JBHI_2021/FANET.png">
    </div>
    <div class="publication_title">
        Accurate Retinal Vessel Segmentation in Color Fundus images via Fully Attention-based Networks </br>
        Kaiqi Li*, <b>Xingqun Qi*</b>, Yiwen Luo, Zeyi Yao, Xiaoguang Zhou, Muyi Sun
</br>
        IEEE Journal of Biomedical and Health Informatics (IEEE JBHI) 2021.</br>
	[<a href="https://ieeexplore.ieee.org/abstract/document/9210783">Paper</a>]
    </div>
</div>

						
<h2><font color="#4682B4">Competitions</font></h2>

<ul>                                                                   
    <li>                                                                        
        <div class="marker">Runner Up of the ChaLearn Sign Spotting Challenge Challenge</div> <div>ECCV, 2022</div>
    </li>                                                                       
</ul>
 
<h2><font color="#4682B4">Services</font></h2>

<ul>                                                                
    <li>
        <div class="marker">Conference Reviewer: CVPR, ICML, NeurIPS, ICLR, ACM MM, WACV, ACCV (outstanding reviewer in 2022)</div>
    </li>                                                                       
    <li>
        <div class="marker">Journal Reviewer: IJCV, TIFS, TNNLS, TMM, TOMM, JBHI, PR, TCBB</div>
    </li>                                                                       
</ul>

</div>

</div>
</body>
</html>
